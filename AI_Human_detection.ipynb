{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-ArunaDevi06/AI_generated_review_detection/blob/main/AI_Human_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vLuMCKhMfp0",
        "outputId": "4166e4c6-ebd7-4c16-9308-8e70e73ec003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall transformers\n",
        "!pip install torch --upgrade\n",
        "!pip install transformers --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "awIwVmktOsGe",
        "outputId": "2f742345-1476-4fe1-9c54-d18457808931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m736.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1\n",
            "    Uninstalling torch-1.13.1:\n",
            "      Successfully uninstalled torch-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "text-classifier-based-openai-community-gpt2 0.1.3 requires tokenizers<0.14.0,>=0.10.0, but you have tokenizers 0.21.0 which is incompatible.\n",
            "text-classifier-based-openai-community-gpt2 0.1.3 requires torch<2.0.0,>=1.7.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              },
              "id": "7ae95cb84efb45e1b9dd6a71843e969a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.48.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwW2hu2BFMlw",
        "outputId": "75770942-9f9a-410f-f058-5cc126758b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.48.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ntgqCm1bMM"
      },
      "source": [
        "Tamil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMMTr_uvmKl-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "78f8572c67fb4b85a89b0932c7c7c62d",
            "e3ca0df70b7c48d7a128b96049637f33",
            "6a4dc19a24924618a48602d3782f1320",
            "d17706909f5547548a4d6312ae4b7ce3",
            "f29f8784939e4b9aba687091bca66b30",
            "02c3cd6efbbd433baa3a2805556e77c3",
            "e2d2d7c8ad084d848eec80c0fbd2e390",
            "bf02a7605d8e45d29daf5bfbf8722338",
            "479066edd2224acb9d2544ad40f38ef5",
            "e34946dddcb3410188784708cf220fde",
            "a6ddf17265ea4d2da5c46fc0b06589d9",
            "f22f7136ef1c41e0a16a4a68ad3439f3",
            "c60690df952c4032a534c4778e5e71a1",
            "a582b74ba72047489773eca8fae8adad",
            "9f70ef7b948244a69f2e144585ddf140",
            "415c61229cf14619b21397fd9ea24dbb",
            "f41e36eb484543599453dad913b4c8ad",
            "ebe358ed64e34193b5004f1faf7362c8",
            "040745e72e9340b1adc3ccdefeed8653",
            "1d39fcfeec2440d0b3f7d51011751e84",
            "f456e07098474247baa90504ae897348",
            "d5b8fcf97bd24dbf9ed7e518559ff3c3",
            "f6d5a63bbad3436689e32c7fe3dd9eae",
            "0a30cf418c7d4e9388cce29e26b5b90a",
            "c3f4a7d2aedf41378baeb95f2f9bfb48",
            "db26f775025a4d2f93f257419272f678",
            "51ad20ad5cf34a1cadf3cf7433d5a73f",
            "bb33c0cead144f9cb1280949d9577dbb",
            "7a385e5ab4e843fb958f7983f2093214",
            "688a846339a2409c9b6454fb0e62a1c9",
            "c4a6cb40abb24a23ad99c959f712085c",
            "8ff586d41af342d485742b4b85e7743c",
            "f4d81e7faf484165a155a5d2888e6f8f",
            "03a58d83a2b4456a8b3f6491511ceaee",
            "76b7024776244df1be8f0c32ed754748",
            "d271bf213c9740ba8d3add8c93de868f",
            "f9f4557a2b804127ac8a484b82046076",
            "fb36040d7d94444488fbf4959125cdb8",
            "f6f8954726184cd7be805bbfaf0f9b94",
            "971b750115b04beea12f737d88c76a99",
            "588ef0cfb751494caf473a118ecde50d",
            "0d86cbb03024470d945b0bac730c8fdc",
            "4f5d43d82b9b412c9c762848718e9f4c",
            "b9b696997ded4036add87100643bd345",
            "59731a981ebb402aafafae32df0db4ed",
            "a48f57166c1e4136b993e3f21da0390d",
            "78b144224a724f4db3bf0b967704f9a7",
            "c36c851c81334b5fbac0f30070b0f61e",
            "52981f26fa404961a45cf0a9512c5918",
            "59cde8ab2ddc42ae916c058bbf1b1f8a",
            "854d47247e0c4f43a0bc292cf1fb13b1",
            "9d3179fc5b0d4cd4b055f4ced972d2dc",
            "8cfa0489b6574e049f7b543d2ef6e66c",
            "4f558581a7dc4a2cb85c7588d43bf465",
            "ed4142301cd14e7fa2f055009b02fe30"
          ]
        },
        "id": "Lz1HzbCnl2iH",
        "outputId": "238ab439-3428-448d-a499-c73b290908d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78f8572c67fb4b85a89b0932c7c7c62d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f22f7136ef1c41e0a16a4a68ad3439f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6d5a63bbad3436689e32c7fe3dd9eae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/606 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03a58d83a2b4456a8b3f6491511ceaee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/202 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59731a981ebb402aafafae32df0db4ed"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load CSV from Google Drive\n",
        "file_path = \"/content/drive/MyDrive/Human_AI_detection/tam_training_data_hum_ai.csv\"  # Adjust your path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing: Extract relevant columns\n",
        "data = data[['DATA', 'LABEL']]\n",
        "data['LABEL'] = data['LABEL'].astype('category').cat.codes  # Convert labels to numeric\n",
        "\n",
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['DATA'], data['LABEL'], test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"ai4bharat/indic-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=data['LABEL'].nunique())\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
        "test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "wG94eRkZl_vd",
        "outputId": "fd049439-e3ab-46e4-f34d-6d406a89e1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-8e6bc979d940>:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [114/114 14:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.543700</td>\n",
              "      <td>0.444697</td>\n",
              "      <td>0.955446</td>\n",
              "      <td>0.955128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.390500</td>\n",
              "      <td>0.383266</td>\n",
              "      <td>0.960396</td>\n",
              "      <td>0.960146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.361136</td>\n",
              "      <td>0.965347</td>\n",
              "      <td>0.965154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=114, training_loss=0.4474204140796996, metrics={'train_runtime': 903.4711, 'train_samples_per_second': 2.012, 'train_steps_per_second': 0.126, 'total_flos': 3406132964520.0, 'train_loss': 0.4474204140796996, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # Updated to eval_strategy\n",
        "    logging_dir='./logs',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\"  # Prevents reporting to W&B\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)  # Use NumPy for logits processing\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Compute macro F1 score\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"macro_f1\": macro_f1}\n",
        "\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load your test dataset\n",
        "test_dataset = pd.read_excel(\"/content/drive/MyDrive/Human_AI_detection/tam_test_data_hum_ai.xlsx\")\n",
        "\n",
        "# Convert the DataFrame to a HuggingFace Dataset\n",
        "test_hf_dataset = Dataset.from_pandas(test_dataset)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/content-type-classifier-deberta\")\n",
        "\n",
        "# Tokenize the test dataset\n",
        "test_hf_dataset = test_hf_dataset.map(lambda x: tokenizer(x['DATA'], truncation=True, padding=True), batched=True)\n",
        "\n",
        "# Get model predictions\n",
        "predictions = trainer.predict(test_hf_dataset)\n",
        "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "\n",
        "# Convert numeric predictions to original labels\n",
        "pred_labels_mapped = []\n",
        "for i in range(len(pred_labels)):\n",
        "  if pred_labels[i]==1:\n",
        "    pred_labels_mapped.append(\"HUMAN\")\n",
        "  else:\n",
        "    pred_labels_mapped.append(\"AI\")\n",
        "\n",
        "\n",
        "print(pred_labels_mapped)\n",
        "# Create a DataFrame with ID and PREDICTION\n",
        "results_df = pd.DataFrame({\n",
        "    \"ID\": test_dataset['ID'],\n",
        "    \"PREDICTION\": pred_labels_mapped\n",
        "})\n",
        "\n",
        "\n",
        "print(results_df)\n",
        "# Save as a TSV file\n",
        "results_df.to_csv(\"tamil_indicBERT_test_results.tsv\", sep='\\t', index=False)\n",
        "\n",
        "print(\"Results saved to 'tamil_indicBERT_test_results.tsv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "4596ca407ed54c72923a824366aca33d",
            "146296c4aa114ed3a09d158834f17a98",
            "8efff04c943c4f17be67083297fe648c",
            "f74cf71f0a87441f920886921a585324",
            "fa97f9b1c1644a108284061fb5cba327",
            "ef40bce9e2ab4996bd042fb19762bcff",
            "0feab6f9f5eb41b08a7a436f3d3de257",
            "0e120074ed0a460e96c1ccf4cc0c92b1",
            "529e04116ec64424b553a79073392bc4",
            "13dca0f8cf05477ab73324d2752b4888",
            "42fce9ac2c6444b1bed56c2e9e924c13"
          ]
        },
        "id": "-6tQJ6HDNBBs",
        "outputId": "084dbda8-6ac2-445c-d90e-eeb660bbb2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4596ca407ed54c72923a824366aca33d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AI', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'HUMAN', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'HUMAN', 'AI', 'HUMAN', 'HUMAN', 'AI', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'HUMAN', 'AI', 'AI', 'HUMAN', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'AI', 'HUMAN', 'AI', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'AI', 'HUMAN', 'AI', 'HUMAN', 'AI', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'AI', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'AI', 'HUMAN', 'HUMAN', 'HUMAN', 'AI']\n",
            "                 ID PREDICTION\n",
            "0   TAM_HUAI_TE_001         AI\n",
            "1   TAM_HUAI_TE_002      HUMAN\n",
            "2   TAM_HUAI_TE_003      HUMAN\n",
            "3   TAM_HUAI_TE_004      HUMAN\n",
            "4   TAM_HUAI_TE_005      HUMAN\n",
            "..              ...        ...\n",
            "95  TAM_HUAI_TE_096         AI\n",
            "96  TAM_HUAI_TE_097      HUMAN\n",
            "97  TAM_HUAI_TE_098      HUMAN\n",
            "98  TAM_HUAI_TE_099      HUMAN\n",
            "99  TAM_HUAI_TE_100         AI\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "Results saved to 'tamil_indicBERT_test_results.tsv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "LKqSmcFxmETI",
        "outputId": "f7f28cb8-d9e3-4bc7-ad83-2d104e907235"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 03:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro F1 Score: 0.9650993459212638\n",
            "Accuracy: {'eval_loss': 0.3436174988746643, 'eval_accuracy': 0.9653465346534653, 'eval_macro_f1': 0.9650993459212638, 'eval_runtime': 34.1562, 'eval_samples_per_second': 5.914, 'eval_steps_per_second': 0.381, 'epoch': 3.0} \n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# Print macro F1 score\n",
        "print(f\"Macro F1 Score: {results['eval_macro_f1']}\")\n",
        "print(f\"Accuracy: {results} \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Malayalam\n"
      ],
      "metadata": {
        "id": "vOaDSbA2LgoR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "92769fa5a5cc47598620402538144751",
            "5eee5fabcc614e949f8e4b0a370aa66d",
            "debbe466bb754945bd09370a0bf7d341",
            "f325a6b88b784b95a3a1bd46313be548",
            "f06e9cf0611e401bb65c0efec39b94b6",
            "c367225d9b9f45d096bcb7f8f4546539",
            "7765cb49bbb4454086478ebb990e5439",
            "99879fbeee174f7ca427700c65e63e46",
            "3644cc9692714d9e9a7fdd0b3e51a6d4",
            "cabdbe580a6f42ccb47e1f663c3c401f",
            "9fec8f0f65214ac3b517359d2fc4326f",
            "aa7716a65eb14827a5e87250470018e6",
            "f77cfc94ddad498c9981629652384eb1",
            "ad4dc5ea4c324bd594dcd6eaa129437b",
            "7dd9ac65900a4db3bf8a31c13cc15919",
            "a3c4e5892b614904a23cf755065c5b38",
            "ae9f808d0e6543bbb523f692aa2e3f45",
            "2f3cd612593d4f84b0c278bcb945eeed",
            "a68987502ea24a7e8e085f678c4f3d4d",
            "61c731e09de1492a811ece911397769e",
            "60a566462adb404fb8ea501b893815f6",
            "fc23c676c6f84f3ebb783817a8b0e080"
          ]
        },
        "id": "F27tDwa-1fAQ",
        "outputId": "1d0e5d6a-adc3-43d9-ffb1-77d842236d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92769fa5a5cc47598620402538144751"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa7716a65eb14827a5e87250470018e6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load CSV from Google Drive\n",
        "file_path = \"/content/drive/MyDrive/Human_AI_detection/mal_training_data_hum_ai.csv\"  # Adjust your path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing: Extract relevant columns\n",
        "data = data[['DATA', 'LABEL']]\n",
        "data['LABEL'] = data['LABEL'].astype('category').cat.codes  # Convert labels to numeric\n",
        "\n",
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['DATA'], data['LABEL'], test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"ai4bharat/indic-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=data['LABEL'].nunique())\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
        "test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "tcmn-CNn1kPE",
        "outputId": "f5c18074-9ce6-4a45-89ae-2cdd3bc4f04c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-8e6bc979d940>:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='112' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [112/114 29:35 < 00:32, 0.06 it/s, Epoch 2.92/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.648000</td>\n",
              "      <td>0.569616</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.387300</td>\n",
              "      <td>0.345607</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.949955</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [114/114 31:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.648000</td>\n",
              "      <td>0.569616</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.387300</td>\n",
              "      <td>0.345607</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.949955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.293100</td>\n",
              "      <td>0.308776</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>0.964929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=114, training_loss=0.46726180064050776, metrics={'train_runtime': 1878.3071, 'train_samples_per_second': 0.958, 'train_steps_per_second': 0.061, 'total_flos': 7852766875200.0, 'train_loss': 0.46726180064050776, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # Updated to eval_strategy\n",
        "    logging_dir='./logs',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\"  # Prevents reporting to W&B\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)  # Use NumPy for logits processing\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Compute macro F1 score\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"macro_f1\": macro_f1}\n",
        "\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DsBhgOS1qQQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "711ede65-9fe8-4166-c969-5e09cf40a8e3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 01:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Macro F1 Score: 0.9649289811869035\n",
            "Accuracy: {'eval_loss': 0.3087761104106903, 'eval_accuracy': 0.965, 'eval_macro_f1': 0.9649289811869035, 'eval_runtime': 76.6079, 'eval_samples_per_second': 2.611, 'eval_steps_per_second': 0.17, 'epoch': 3.0} \n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# Print macro F1 score\n",
        "print(f\"Macro F1 Score: {results['eval_macro_f1']}\")\n",
        "print(f\"Accuracy: {results} \")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load your test dataset\n",
        "test_dataset = pd.read_excel(\"/content/drive/MyDrive/Human_AI_detection/mal_test_data_hum_ai.xlsx\")\n",
        "\n",
        "# Convert the DataFrame to a HuggingFace Dataset\n",
        "test_hf_dataset = Dataset.from_pandas(test_dataset)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/content-type-classifier-deberta\")\n",
        "\n",
        "# Tokenize the test dataset\n",
        "test_hf_dataset = test_hf_dataset.map(lambda x: tokenizer(x['DATA'], truncation=True, padding=True), batched=True)\n",
        "\n",
        "# Get model predictions\n",
        "predictions = trainer.predict(test_hf_dataset)\n",
        "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "print(pred_labels)\n",
        "\n",
        "# Convert numeric predictions to original labels\n",
        "pred_labels_mapped = []\n",
        "for i in pred_labels:\n",
        "  if i==1:\n",
        "    pred_labels_mapped.append(\"HUMAN\")\n",
        "  else:\n",
        "    pred_labels_mapped.append(\"AI\")\n",
        "\n",
        "\n",
        "print(pred_labels_mapped)\n",
        "# Create a DataFrame with ID and PREDICTION\n",
        "results_df = pd.DataFrame({\n",
        "    \"ID\": test_dataset['ID'],\n",
        "    \"PREDICTION\": pred_labels_mapped\n",
        "})\n",
        "\n",
        "\n",
        "# Save as a TSV file\n",
        "results_df.to_csv(\"malayalam_indicBERT_test_results.tsv\", sep='\\t', index=False)\n",
        "\n",
        "print(\"Results saved to 'malayalam_indicBERT_test_results.tsv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "f7d9f64044404663ba542284cc468e03",
            "63a38e4889864462bc3931be2f481f30",
            "ab10000c9e0544ecb71260516d425dea",
            "5db3025a56b14155a3270b7c1e5847f0",
            "e6f85589028640f192d3a4096f66a6d4",
            "d54abf1097dd427c842a2951744cbe21",
            "de8b590af1914dffa5eca3f27799f180",
            "1244d02772cc4a2dbcc8b3809979a1c0",
            "6ea784295560424ba7896fa8c2cf0bda",
            "b0636320d4fb499fa142fc2fb1bf7b91",
            "d15e218b786a46c9bcbb7dae86b7b26b"
          ]
        },
        "id": "mRgjSkkI1Nqk",
        "outputId": "65ecfca6-5bdf-4cbc-c629-e9f4378ab36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7d9f64044404663ba542284cc468e03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "['HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN', 'HUMAN']\n",
            "Results saved to 'malayalam_indicBERT_test_results.tsv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3BFZGJaDAcL",
        "outputId": "f23e58bb-6fe7-4029-80cf-2c141a2f080c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  DATA  LABEL\n",
            "0    ഞാൻ കുറച്ച് കാലമായി മുച്ചട്ച്ചിൻ്റെ ഫേസ് വാഷ് ...      1\n",
            "1             ഈ ഫേസ് വാഷ് തണുപ്പ് വെതറിലും ഉപയോഗിക്കാം      1\n",
            "2    അണ്ണാ എനിക്ക് 14 വയസ് ആയ തേയോളു എനിക്ക് സ്കിൻക...      1\n",
            "3    ബ്രോ ഇതെല്ലം യൂസ്  ആക്കീട്ട് നൈറ്റ് പിന്നെ വേറ...      1\n",
            "4      ഇത് ഫേസ് വാഷ് ഡെയിലി ചെയ്താ സ്കിൻകെയറിന് നല്ലതാ      1\n",
            "..                                                 ...    ...\n",
            "795  ബിരിയാണി, പപ്പടം, അച്ചാർ - മറ്റെവിടെയും കിട്ടാ...      0\n",
            "796  എങ്കിലും, തട്ടുകടയിലെ ഭക്ഷണത്തിന്റെ സുഖം മറ്റൊ...      0\n",
            "797  പോറോട്ട, ബീഫ് കറി, സാലഡ് - ഈ കോമ്പിനേഷനിൽ നിന്...      0\n",
            "798  നല്ല ഉഴുന്നുവട്ടിയും, കിടിലൻ ചമ്മന്തിയും ചേർന്...      0\n",
            "799  അങ്ങോട്ട് പോയാൽ, ചിക്കൻ കറി, ചപ്പാത്തി ഒന്നും ...      0\n",
            "\n",
            "[800 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SUPPORT VECTOR MACHINE**\n"
      ],
      "metadata": {
        "id": "WiJQ2zrbVdRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Load CSV from Google Drive\n",
        "file_path = \"/content/drive/MyDrive/Human_AI_detection/tam_training_data_hum_ai.csv\"  # Adjust your path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing: Extract relevant columns\n",
        "data = data[['DATA', 'LABEL']]\n",
        "data['LABEL'] = data['LABEL'].astype('category').cat.codes  # Convert labels to numeric\n",
        "\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_df=0.7)\n",
        "\n",
        "# Transform the text data to feature vectors\n",
        "X = vectorizer.fit_transform(data['DATA'])\n",
        "\n",
        "# Labels\n",
        "y = data['LABEL']\n",
        "\n",
        "\n",
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pHi770JIYPCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(train_texts,train_labels)\n",
        "\n",
        "y_pred = clf.predict(test_texts)\n",
        "\n",
        "accuracy = accuracy_score(test_labels, y_pred)\n",
        "f1 = f1_score(test_labels, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dVXYP8CWOtC",
        "outputId": "ad4a499e-4941-455a-9d4f-88bc0b58146a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8465346534653465\n",
            "F1 Score: 0.8465007966662581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tamil svm"
      ],
      "metadata": {
        "id": "fbgcS1IqAFRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load your datasets\n",
        "train_dataset = pd.read_csv(\"/content/drive/MyDrive/Human_AI_detection/tam_training_data_hum_ai.csv\")\n",
        "test_dataset = pd.read_excel(\"/content/drive/MyDrive/Human_AI_detection/tam_test_data_hum_ai.xlsx\")\n",
        "\n",
        "# Vectorize the train and test data\n",
        "vectorizer = TfidfVectorizer(max_df=0.7)\n",
        "\n",
        "# Fit the vectorizer on the train data\n",
        "X_train = vectorizer.fit_transform(train_dataset['DATA'])\n",
        "y_train = train_dataset['LABEL']\n",
        "\n",
        "# Transform the test data\n",
        "X_test = vectorizer.transform(test_dataset['DATA'])\n",
        "\n",
        "# Train the SVM model\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Convert numeric predictions to original labels\n",
        "#pred_labels_mapped = [\"HUMAN\" if i == 1 else \"AI\" for i in y_pred]\n",
        "\n",
        "# Create a DataFrame with ID and PREDICTION\n",
        "results_df = pd.DataFrame({\n",
        "    \"ID\": test_dataset['ID'],\n",
        "    \"PREDICTION\": y_pred\n",
        "})\n",
        "\n",
        "# Save the results as a TSV file\n",
        "results_df.to_csv(\"tamil_SVM_test_results.tsv\", sep='\\t', index=False)\n",
        "\n",
        "print(\"Results saved to 'tamil_SVM_test_results.tsv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8uI-JeMZbpx",
        "outputId": "bce1a76a-44db-4dd4-ef07-a1c44ba6faed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'tamil_SVM_test_results.tsv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "malayalam svm"
      ],
      "metadata": {
        "id": "90B9mggBAH9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Load CSV from Google Drive\n",
        "file_path = \"/content/drive/MyDrive/Human_AI_detection/mal_training_data_hum_ai.csv\"  # Adjust your path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing: Extract relevant columns\n",
        "data = data[['DATA', 'LABEL']]\n",
        "data['LABEL'] = data['LABEL'].astype('category').cat.codes  # Convert labels to numeric\n",
        "\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_df=0.7)\n",
        "\n",
        "# Transform the text data to feature vectors\n",
        "X = vectorizer.fit_transform(data['DATA'])\n",
        "\n",
        "# Labels\n",
        "y = data['LABEL']\n",
        "\n",
        "\n",
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(train_texts,train_labels)\n",
        "\n",
        "y_pred = clf.predict(test_texts)\n",
        "\n",
        "accuracy = accuracy_score(test_labels, y_pred)\n",
        "f1 = f1_score(test_labels, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i01KHqfuVHwJ",
        "outputId": "3a05426b-1998-4edc-ba8d-7bb6bc190660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.775\n",
            "F1 Score: 0.7748592870544091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load your datasets\n",
        "train_dataset = pd.read_csv(\"/content/drive/MyDrive/Human_AI_detection/mal_training_data_hum_ai.csv\")\n",
        "test_dataset = pd.read_excel(\"/content/drive/MyDrive/Human_AI_detection/mal_test_data_hum_ai.xlsx\")\n",
        "\n",
        "# Vectorize the train and test data\n",
        "vectorizer = TfidfVectorizer(max_df=0.7)\n",
        "\n",
        "# Fit the vectorizer on the train data\n",
        "X_train = vectorizer.fit_transform(train_dataset['DATA'])\n",
        "y_train = train_dataset['LABEL']\n",
        "\n",
        "# Transform the test data\n",
        "X_test = vectorizer.transform(test_dataset['DATA'])\n",
        "\n",
        "# Train the SVM model\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Convert numeric predictions to original labels\n",
        "#pred_labels_mapped = [\"HUMAN\" if i == 1 else \"AI\" for i in y_pred]\n",
        "\n",
        "# Create a DataFrame with ID and PREDICTION\n",
        "results_df = pd.DataFrame({\n",
        "    \"ID\": test_dataset['ID'],\n",
        "    \"PREDICTION\": y_pred\n",
        "})\n",
        "\n",
        "# Save the results as a TSV file\n",
        "results_df.to_csv(\"malayalam_SVM_test_results.tsv\", sep='\\t', index=False)\n",
        "\n",
        "print(\"Results saved to 'malayalam_SVM_test_results.tsv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uikC069JAKBr",
        "outputId": "a850e1b2-fd1c-4a0d-9cbe-6c0a4057b851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'malayalam_SVM_test_results.tsv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METRICS FOR TEST DATA AFTER THE LABELS HAVE BEEN PROVIDED"
      ],
      "metadata": {
        "id": "3540jzQc36h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "\n",
        "tam_actual_df = pd.read_csv('/content/drive/MyDrive/Human_AI_detection/tam_test_with_labels.csv')\n",
        "mal_actual_df=  pd.read_csv('/content/drive/MyDrive/Human_AI_detection/mal_test_with_labels.csv')\n",
        "tam_actual_values = tam_actual_df['Label']\n",
        "mal_actual_values = mal_actual_df['Label']\n",
        "\n",
        "#indicBert\n",
        "print(\"\\nIndicBERT\\n\")\n",
        "tam_predicted_df = pd.read_csv('/content/drive/MyDrive/Human_AI_detection/AnalysisArchitectsindicBERT_tamil_run.tsv', sep='\\t')\n",
        "mal_predicted_df = pd.read_csv('/content/drive/MyDrive/Human_AI_detection/AnalysisArchitectsindicBERT_malayalam_run.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "tam_predicted_values = tam_predicted_df['PREDICTION']\n",
        "mal_predicted_values = mal_predicted_df['PREDICTION']\n",
        "\n",
        "\n",
        "# macro F1 score\n",
        "tam_macro_f1 = f1_score(tam_actual_values, tam_predicted_values, average='macro')\n",
        "mal_macro_f1 = f1_score(mal_actual_values, mal_predicted_values, average='macro')\n",
        "# Calculate the accuracy\n",
        "tam_accuracy = accuracy_score(tam_actual_values, tam_predicted_values)\n",
        "mal_accuracy = accuracy_score(mal_actual_values,mal_predicted_values)\n",
        "\n",
        "print(f\"Tamil macro F1 Score : {tam_macro_f1}\")\n",
        "print(f\"Tamil Accuracy : {tam_accuracy}\")\n",
        "\n",
        "print(f\"Malayalam macro F1 Score : {mal_macro_f1}\")\n",
        "print(f\"Malayalam Accuracy : {mal_accuracy}\")\n",
        "\n",
        "\n",
        "#SVM\n",
        "print(\"\\nSVM\\n\")\n",
        "tam_predicted_df = pd.read_csv('/content/drive/MyDrive/Human_AI_detection/AnalysisArchitectsSVM_tamil_run.tsv', sep='\\t')\n",
        "mal_predicted_df = pd.read_csv('/content/drive/MyDrive/Human_AI_detection/AnalysisArchitectsSVM_malayalam_run.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "tam_predicted_values = tam_predicted_df['PREDICTION']\n",
        "mal_predicted_values = mal_predicted_df['PREDICTION']\n",
        "\n",
        "\n",
        "# macro F1 score\n",
        "tam_macro_f1 = f1_score(tam_actual_values, tam_predicted_values, average='macro')\n",
        "mal_macro_f1 = f1_score(mal_actual_values, mal_predicted_values, average='macro')\n",
        "# Calculate the accuracy\n",
        "tam_accuracy = accuracy_score(tam_actual_values, tam_predicted_values)\n",
        "mal_accuracy = accuracy_score(mal_actual_values,mal_predicted_values)\n",
        "\n",
        "print(f\"Tamil macro F1 Score : {tam_macro_f1}\")\n",
        "print(f\"Tamil Accuracy : {tam_accuracy}\")\n",
        "\n",
        "print(f\"Malayalam macro F1 Score : {mal_macro_f1}\")\n",
        "print(f\"Malayalam Accuracy : {mal_accuracy}\")\n",
        "\n",
        "\n",
        "#AlBert\n",
        "print(\"\\nAlBERT\\n\")\n",
        "tam_predicted_df = pd.read_csv('/content/drive/MyDrive/Human_AI_detection/AnalysisArchitectsALBERT_tamil_run.tsv', sep='\\t')\n",
        "mal_predicted_df = pd.read_csv('/content/drive/MyDrive/Human_AI_detection/AnalysisArchitectsALBERT_malayalam_run.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "tam_predicted_values = tam_predicted_df['PREDICTED_LABEL']\n",
        "mal_predicted_values = mal_predicted_df['PREDICTED_LABEL']\n",
        "\n",
        "\n",
        "# macro F1 score\n",
        "tam_macro_f1 = f1_score(tam_actual_values, tam_predicted_values, average='macro')\n",
        "mal_macro_f1 = f1_score(mal_actual_values, mal_predicted_values, average='macro')\n",
        "# Calculate the accuracy\n",
        "tam_accuracy = accuracy_score(tam_actual_values, tam_predicted_values)\n",
        "mal_accuracy = accuracy_score(mal_actual_values,mal_predicted_values)\n",
        "\n",
        "print(f\"Tamil macro F1 Score: {tam_macro_f1}\")\n",
        "print(f\"Tamil Accuracy : {tam_accuracy}\")\n",
        "\n",
        "print(f\"Malayalam macro F1 Score : {mal_macro_f1}\")\n",
        "print(f\"Malayalam Accuracy : {mal_accuracy}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_uhyKlB0Svp",
        "outputId": "35ec04a7-ddc7-4335-8911-8d79f5c209e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IndicBERT\n",
            "\n",
            "Tamil macro F1 Score : 0.62\n",
            "Tamil Accuracy : 0.62\n",
            "Malayalam macro F1 Score : 0.3333333333333333\n",
            "Malayalam Accuracy : 0.5\n",
            "\n",
            "SVM\n",
            "\n",
            "Tamil macro F1 Score : 0.6594551282051282\n",
            "Tamil Accuracy : 0.66\n",
            "Malayalam macro F1 Score : 0.6797117405665098\n",
            "Malayalam Accuracy : 0.68\n",
            "\n",
            "AlBERT\n",
            "\n",
            "Tamil macro F1 Score: 0.4375\n",
            "Tamil Accuracy : 0.46\n",
            "Malayalam macro F1 Score : 0.8849741191768148\n",
            "Malayalam Accuracy : 0.885\n"
          ]
        }
      ]
    }
}
